\documentclass[12pt,a4paper]{article}
% -- LuaLaTeX --

\usepackage{qsettings}

% настройки титульной страницы
\doctype{REP}


%\nofiles

%\directlua{ require("drawboxes")}\usepackage{atbegshi}\AtBeginShipout {\directlua{drawboxes.visual_debug()}}

\year=2017

% ---------- НАЧАЛО ДОКУМЕНТА ----------

\begin{document}

{\raggedleft \Large Гетманенко Глеб, гр. 4645М. Тест 2 \par}

\hrule

\section{Слои Кохонена и Гроссберга в сети встречного распространения. Их найстройка.}

Слои Кохонена и Гроссберга являются видами полносвязных слоев. Слой Кохонена
функционирует по принципу ``победитель забирает все'': нейрон с максимальным уровнем
возбуждения (сумма произведений весов на входы нейрона) выдает на выходе
логическую единицу, а на остальных выходах выдается ноль. Выходом
слоя Гроссберга является взвешенная сумма выходов нейроново слоя Кохонена.
Фактически, каждый нейрон слоя Гроссберга выдает вес связи между ним и единственным
активным нейроном слоя Кохонена.

В процессе обучения сети встречного распространения слой Кохонена получается,
что слой Кохонена выполняет кластеризацию входных данных. Это достигается с помощью
такой подстройки весовых коэффициентов слоя Кохонена, что близкие в пространстве
признаков входные векторы активизруют один и тот же нейрон данного слоя. Задачей
слоя Гроссберга является получение требуемого выходного вектора. В процессе обучения
необходимо гарантировать, чтобы в результате обучения непохожие входные векторы
активизировали разные нейроны слоя Кохонена.

Перед обучением сети необходимо задать слою Кохонена начальные значения.
Для предотвращения переобучения, при котором образы одного класса активируют
несколько различных нейронов, или, наоборот, недообучения, при котором образы
из нескольких классов активизируют один и тот же нейрон, используется метод
выпуклой комбинации: веса связей всех нейронов слоя Кохонена при инициализации
приравниваются одной и той же величине: $ w_{ij} = \dfrac{1}{\sqrt{n}} $,
где $n$ - число входов. А при обучении каждой компоненте входного вектора $X$
из разделяемого множества придается значение
$x_i = \alpha x_i + \frac{1-\alpha}{\sqrt{n}}$. В начале обучения $\alpha$
очень мало, из-за чего все входные векторы имеют длину близкую к $\dfrac{1}{\sqrt{n}}$
и почти совпадают с векторами весовых коэффициентов. В процессе обучения 
$\alpha$ увеличивается вплоть до единицы. Это позволяет постепенно разделять
входные векторы и окончательно приписывает им их истинные значения. Этот
метод хорошо работает, но замедляет процесс обучения.

Настройка весов слоя Гроссберга представляет собой установку значений весов
тех связей, которые связывают нейроны слоя с единственным ненулевым в данный
момент ненулевым нейроном слоя Кохонена, соответствующим входному вектору $X$ так,
чтобы на выходе сети получить требуемый вектор $y$.

\section{Укажите порядок просмотра узлов ``жадным'' алгоритмом (цель - узел W).}

\easypic{./test2-pic.png}{}{2.8}

\begin{enumerate}
    \item Оценка начального состояния A; А - не цель.
    \item Потомки A: B-4, C-6, D-8; closed: A.
    \item Потомки B: E-3, F-5; closed: A, B.
    \item Потомки C: G-4, H-7; closed: A, B, C.
    \item E - оптимальней. Потомки E: K-2, L-2; closed: A, B, C, E.
    \item Потомки K: S-1. S - не цель, потомков нет; closed: A, B, C, E, K, S.
    \item Потомки L: T-1. T - не цель, потомков нет; closed: A, B, C, E, K, S, L.
    \item G - менее оптимальный. Потомки G: N-6, O-2; closed: A, B, C, E, K, S, L, T, G.
    \item O - не цель, потомков нет; closed: A, B, C, E, K, S, L, T, G, O.
    \item Потомки N: U-2, V-5; closed: A, B, C, E, K, S, L, T, G, O, N.
    \item U - не цель, потомков нет; V - не цель, потомков нет; closed: A, B, C, E, K, S, L, T, G, O, N.
    \item Потомки F: L - тут были, M-3; closed: A, B, C, E, K, S, L, T, G, O, N, U, V, F.
    \item M - не цель, потомков нет; closed: A, B, C, E, K, S, L, T, G, O, N, U, V, F.
    \item Потомки I: P-9, Q-6; closed: A, B, C, E, K, S, L, T, G, O, N, U, V, F, I.
    \item Q - не цель, потомков нет; closed: A, B, C, E, K, S, L, T, G, O, N, U, V, F, I.
    \item Потомки P: W-10; closed: closed: A, B, C, E, K, S, L, T, G, O, N, U, V, F, I, Q, P.
    \item W - цель найдена.
\end{enumerate}

\section{Задача}

В двумерном пространстве признаков заданы 4 точки $A(-2, 20)$, $B(7, -3)$, $C(15, 3)$, $D(11, 8)$.
Найдите нормированное Манхэттенское расстояние между точками $A$ и $B$.

Найдем нормированные значения признаков каждой точки по формуле: 
\[  x' = \dfrac{x-x_{min}}{x_{max} - x_{min}}. \]

\begin{center}
  \begin{tabular}{|c|l|l|l|}
    \hline
      & $x'$ & $y'$ \\\hline
    A & 0.00 & 1.00 \\\hline
    B & 0.53 & 0.00 \\\hline
    C & 1.00 & 0.26 \\\hline
    D & 0.76 & 0.48 \\\hline
  \end{tabular}
\end{center}

Найти манхеттенское расстояние между точками можно по формуле:
\[ d_{lp} = \sum_{i=1}^{n} \left| x_{il} - x_{ip} \right|. \]

Вычислим расстояние между $A$ и $B$:

\[ d_{AB} = |A_x - B_x| + |A_y - B_y| \approx 0.53 + 1 \approx 1.53 \]

\end{document}
